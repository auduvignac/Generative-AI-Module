{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d979863c",
   "metadata": {},
   "source": [
    "# IA gÃ©nÃ©rative : Concepts, Outils, cas d'utilisation\n",
    "\n",
    "## Intro Ã  l'IA gÃ©nÃ©rative\n",
    "\n",
    "L'IA gÃ©nÃ©rative est un sous-domaine de l'intelligence artificielle qui se concentre sur la crÃ©ation de contenu original, que ce soit du texte, des images, de la musique ou d'autres formes de mÃ©dias. Contrairement aux systÃ¨mes d'IA traditionnels qui se contentent d'analyser et de classer des donnÃ©es existantes, l'IA gÃ©nÃ©rative utilise des algorithmes avancÃ©s pour produire de nouvelles crÃ©ations basÃ©es sur des modÃ¨les appris Ã  partir de grandes quantitÃ©s de donnÃ©es.\n",
    "\n",
    "Exemple d'outils : \n",
    "- ChatGPT : gÃ©nÃ©ration de texte conversationnel et trÃ¨s gÃ©nÃ©raliste ;\n",
    "- DALL-E : gÃ©nÃ©ration d'images Ã  partir de descriptions textuelles ;\n",
    "- Perplexity : recherche sur le web (l'indexation sur le web est optimisÃ© par rapport Ã  chatgpt) ;\n",
    "\n",
    "Les Ia gÃ©nÃ©ratives il y a deux ans Ã©taient basÃ©s sur des input/output textuels. Aujourd'hui, on peut gÃ©nÃ©rer des images, de la musique, des vidÃ©os, etc. Ã  partir de texte, d'image, de son, de video, etc.\n",
    "\n",
    "### Machine Learning vs Generative AI\n",
    "\n",
    "Le machine learning est un sous-domaine de l'IA qui se concentre sur l'apprentissage Ã  partir de donnÃ©es. Il utilise des algorithmes pour identifier des modÃ¨les et faire des prÃ©dictions basÃ©es sur ces modÃ¨les. L'IA gÃ©nÃ©rative, quant Ã  elle, va au-delÃ  de la simple analyse des donnÃ©es en crÃ©ant de nouvelles donnÃ©es qui imitent les caractÃ©ristiques des donnÃ©es d'entraÃ®nement.\n",
    "L'IA gÃ©nÃ©rative utilise souvent des techniques de machine learning, mais elle se concentre sur la crÃ©ation plutÃ´t que sur l'analyse.\n",
    "\n",
    "$$\n",
    "\\text{Input} \\rightarrow \\text{Machine Learning} \\rightarrow \\text{Output}\n",
    "$$\n",
    "\n",
    "Machine Learning : On part d'un modÃ¨le prÃ© entraÃ®nÃ© en vue de faire du *fine tuning*.\n",
    "\n",
    "Tandis que l'IA gÃ©nÃ©rative part d'un modÃ¨le prÃ© entraÃ®nÃ© et va gÃ©nÃ©rer de nouvelles donnÃ©es (textes, images, sons, etc.) Ã  partir de ce modÃ¨le et en rÃ©ponse Ã  des *prompt*.\n",
    "Les modÃ¨les d'IA gÃ©nÃ©rtaive sont trÃ¨s bons pour :\n",
    "- zero short learning : quelques donnÃ©es d'entraÃ®nement suffisent pour entraÃ®ner un modÃ¨le pour faire une tÃ¢che ;\n",
    "- few short learning : quelques exemples en input dans le prompt suffisent pour faire une tÃ¢che ;\n",
    "\n",
    "Le modÃ¨le va ainsi mieux comprendre ce qu'on attend de lui Ã  partir des exemples fournis.\n",
    "\n",
    "| Aspect                           | Classic Machine Learning                                                                 | Generative AI                                                                                   |\n",
    "|----------------------------------|-------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------|\n",
    "| **Objective**                    | Learn patterns from labeled data to **predict outputs**                                   | Learn from large data to **generate new content**                                               |\n",
    "| **Training**                     | Usually trained on a **task-specific** dataset (e.g., spam detection, image classification) | Pre-trained on **massive general-purpose datasets** (e.g., Common Crawl, Wikipedia)             |\n",
    "| **Learning Paradigm**           | **Supervised learning** is common (with labels), but unsupervised and reinforcement also used | Typically uses **unsupervised/self-supervised** learning, sometimes fine-tuned                 |\n",
    "| **Interaction with New Data**   | Model must be **trained or re-trained** on specific data                                  | Can work **zero-shot, few-shot**, or be **fine-tuned** with prompts or examples                |\n",
    "| **Input â†’ Output**              | Input data â†’ label or prediction (e.g., input image â†’ cat)                                | Prompt (task + optional examples) â†’ generated output (e.g., text, image, code)                  |\n",
    "| **Examples**                    | Linear regression, Random Forest, SVM, XGBoost                                            | GPT-4, Claude, Stable Diffusion, Gemini                                                         |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187651d",
   "metadata": {},
   "source": [
    "| Aspect                          | Apprentissage Automatique Classique                                                      | Intelligence Artificielle GÃ©nÃ©rative                                                          |\n",
    "|----------------------------------|-------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------|\n",
    "| **Objectif**                     | Apprendre des motifs Ã  partir de donnÃ©es Ã©tiquetÃ©es pour **prÃ©dire des rÃ©sultats**        | Apprendre Ã  partir de grandes quantitÃ©s de donnÃ©es pour **gÃ©nÃ©rer du contenu**                |\n",
    "| **EntraÃ®nement**                 | GÃ©nÃ©ralement entraÃ®nÃ© sur un **jeu de donnÃ©es spÃ©cifique Ã  une tÃ¢che**                    | PrÃ©-entraÃ®nÃ© sur des **jeux de donnÃ©es gÃ©nÃ©ralistes massifs** (ex. : Common Crawl, Wikipedia) |\n",
    "| **Paradigme d'apprentissage**   | Lâ€™**apprentissage supervisÃ©** est courant (avec Ã©tiquettes), mais l'apprentissage non supervisÃ© et par renforcement sont aussi utilisÃ©s | Utilise gÃ©nÃ©ralement l'**apprentissage non supervisÃ© ou auto-supervisÃ©**, parfois avec ajustement (fine-tuning) |\n",
    "| **Interaction avec nouvelles donnÃ©es** | Le modÃ¨le doit Ãªtre **entraÃ®nÃ© ou rÃ©-entraÃ®nÃ©** sur des donnÃ©es spÃ©cifiques              | Peut fonctionner en **zero-shot, few-shot**, ou Ãªtre **ajustÃ©** avec des prompts ou exemples   |\n",
    "| **EntrÃ©e â†’ Sortie**             | DonnÃ©e d'entrÃ©e â†’ Ã©tiquette ou prÃ©diction (ex. : image â†’ chat)                           | Prompt (tÃ¢che + exemples optionnels) â†’ contenu gÃ©nÃ©rÃ© (ex. : texte, image, code)              |\n",
    "| **Exemples**                    | RÃ©gression linÃ©aire, ForÃªt alÃ©atoire, SVM, XGBoost                                       | GPT-4, Claude, Stable Diffusion, Gemini                                                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d525d",
   "metadata": {},
   "source": [
    "|                        | **Trained ML Model**                                                                 | **LLM Zero-Shot Categorizer**                                                  |\n",
    "|------------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| âœ… **Pros**             | - High precision when well-trained                                                    | - No need to gather labeled data                                               |\n",
    "|                        | - Handles subtle distinctions (e.g., *Running* vs *Training* shoes)                   | - Fast to deploy for new customers                                             |\n",
    "|                        | - Can return prediction confidence                                                    | - Easy to test & iterate with just prompts                                     |\n",
    "| âŒ **Cons**             | - Requires labeled data per customer                                                  | - Can struggle with close categories                                           |\n",
    "|                        | - Costly training & retraining                                                        | - No probability/confidence score                                              |\n",
    "|                        | - Infra needed: model storage, monitoring, scaling                                    | - May degrade if category list is large or complex                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c02803",
   "metadata": {},
   "source": [
    "|                        | **ModÃ¨le ML EntraÃ®nÃ©**                                                                | **CatÃ©goriseur LLM Zero-Shot**                                                 |\n",
    "|------------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| âœ… **Avantages**        | - Haute prÃ©cision lorsquâ€™il est bien entraÃ®nÃ©                                         | - Pas besoin de collecter des donnÃ©es Ã©tiquetÃ©es                               |\n",
    "|                        | - GÃ¨re les distinctions subtiles (ex. : *chaussures de course* vs *chaussures dâ€™entraÃ®nement*) | - DÃ©ploiement rapide pour de nouveaux clients                                  |\n",
    "|                        | - Peut retourner un score de confiance                                                | - Facile Ã  tester et itÃ©rer avec de simples prompts                            |\n",
    "| âŒ **InconvÃ©nients**    | - NÃ©cessite des donnÃ©es Ã©tiquetÃ©es pour chaque client                                 | - Peut avoir du mal avec des catÃ©gories proches                                |\n",
    "|                        | - EntraÃ®nement et rÃ©entraÃ®nement coÃ»teux                                              | - Pas de score de probabilitÃ©/confiance                                        |\n",
    "|                        | - Infrastructure nÃ©cessaire : stockage, surveillance, mise Ã  lâ€™Ã©chelle                | - Peut se dÃ©grader si la liste des catÃ©gories est longue ou complexe           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d71e6",
   "metadata": {},
   "source": [
    "## Focus sur les LLM : architectures et fonctionnement\n",
    "\n",
    "![Preprocessing and Vectorization](Figures/NLP_0.png)\n",
    "\n",
    "### 1. Preprocessing\n",
    "\n",
    "This is the first step where raw text is cleaned and prepared for further processing:\n",
    "\n",
    "- **Tokenization**: Splitting text into individual units (words, phrases, symbols).  \n",
    "  - *Example*: `\"The cat sat\"` â†’ `[\"The\", \"cat\", \"sat\"]`\n",
    "\n",
    "- **Lemmatization**: Converting words to their base or dictionary form.  \n",
    "  - *Example*: `\"running\"` â†’ `\"run\"`\n",
    "\n",
    "- **Stop words removal**: Removing common words that donâ€™t add much meaning.  \n",
    "  - *Example*: `\"is\"`, `\"the\"`, `\"and\"`\n",
    "\n",
    "These steps help reduce noise and dimensionality in the text data.\n",
    "\n",
    "### 2. Vectorization\n",
    "\n",
    "After preprocessing, textual data must be converted into numerical format so algorithms can process it:\n",
    "\n",
    "- **Word Embedding**: Representing each word as a dense vector in a semantic space (e.g., Word2Vec, GloVe).  \n",
    "  - *Example*: `\"king\"` and `\"queen\"` have similar vectors due to their contextual usage.\n",
    "\n",
    "- **Sentence Embedding**: Representing entire sentences or paragraphs as a single dense vector.  \n",
    "  - Useful for capturing meaning beyond individual words.\n",
    "\n",
    "### Embeding\n",
    "\n",
    "> **Les embeddings** sont des reprÃ©sentations numÃ©riques du texte oÃ¹ les mots similaires ont des reprÃ©sentations similaires.  \n",
    "> Ils sont indispensables pour que les algorithmes de machine learning comprennent le texte.\n",
    "\n",
    "![Embedding](Figures/embedding_exemple.png)\n",
    "\n",
    "Chacun des vecteurs ci-dessus sera *labellisÃ©s* en vue de faire de l'apprentissage supervisÃ©.\n",
    "- **Word2Vec**: Trains a model to predict a word based on its context (Skip-Gram) or vice versa (CBOW).\n",
    "![Embedding](Figures/word2Vec.png)\n",
    "\n",
    "\n",
    "- **GloVe**: Uses global word co-occurrence statistics to create embeddings.\n",
    "- **FastText**: Similar to Word2Vec but considers subword information, making it effective for morphologically rich languages.\n",
    "- **BERT**: Bidirectional Encoder Representations from Transformers. It uses a transformer architecture to understand the context of words in a sentence.\n",
    "- **Sentence Transformers**: Extends BERT to create embeddings for sentences or paragraphs, useful for tasks like semantic similarity.\n",
    "- **Universal Sentence Encoder**: A model that encodes sentences into high-dimensional vectors, optimized for various NLP tasks.\n",
    "- **OpenAI Embeddings**: Embeddings provided by OpenAI's models, optimized for various tasks.\n",
    "\n",
    "\n",
    "### ModÃ¨le d'embedding Ada (OpenAI) :\n",
    "\n",
    "- A Ã©tÃ© entrÃ¢inÃ© sur plusieurs langues ;\n",
    "- Capture le sens sÃ©mantique du texte, mÃªme Ã  travers diffÃ©rentes langues ; \n",
    "- Fournit des reprÃ©sentations contextuelles des mots, phrases ou documents.\n",
    "\n",
    "#### Comparaison des embeddings\n",
    "\n",
    "- Utilisation du Cosine Similarity pour Ã©valuer la similaritÃ© entre deux vecteurs d'embedding.\n",
    "- Si le Cosine Similarity est proche de 1, cela signifie que les deux vecteurs sont trÃ¨s similaires.\n",
    "- Si en revanche le Cosine Similarity est proche de 0, cela signifie que les deux vecteurs sont trÃ¨s diffÃ©rents (i.e. orthogonaux).\n",
    "\n",
    "![Embedding Cosine](Figures/embedding_cosine.png)\n",
    "\n",
    "**Exemple :** Common Crawl : rÃ©cupÃ¨re tous les sites web du monde entier et les indexe (tous les mois).\n",
    "\n",
    "![Fichier d'exemple Common Crawl](Figures/common_crawl_exemple.png)\n",
    "\n",
    "### 3. Pre-Training\n",
    "\n",
    "Pre training est la phase initiale oÃ¹ le modÃ¨le va apprendre les *patterns*, raisonnements, *knowledge*.\n",
    "\n",
    "![LLM pre training exemple](Figures/LLM_pre_training_0.png)\n",
    "\n",
    "Semi supervisÃ© : le *dataset* d'entraÃ®nement a Ã©tÃ© gÃ©nÃ©rÃ© automatiquement.\n",
    "\n",
    "#### 1. Forward pass\n",
    "\n",
    "##### 1. Propagation avant dans le Transformer\n",
    "\n",
    "- Les tokens embarquÃ©s passent Ã  travers plusieurs **couches de Transformer**, qui :\n",
    "  - Utilisent la **self-attention** pour capturer le contexte sur toute la sÃ©quence et Â« prÃªter attention Â» Ã  certaines parties du texte.\n",
    "  - Appliquent des **rÃ©seaux neuronaux feedforward** pour enrichir les reprÃ©sentations.\n",
    "\n",
    "- Cela donne des **embeddings contextualisÃ©s** pour chaque token.\n",
    "\n",
    "![LLM pre training exemple](Figures/LLM_pre_training_1.png)\n",
    "\n",
    "##### 2. PrÃ©diction (couche de sortie)\n",
    "\n",
    "- Chaque embedding contextualisÃ© est envoyÃ© dans une **couche dense (linÃ©aire)**.\n",
    "- Le modÃ¨le produit une **distribution de probabilitÃ©** sur le vocabulaire pour prÃ©dire le **prochain token**.\n",
    "\n",
    "##### 3. Calcul de la perte (Loss)\n",
    "\n",
    "- Le token prÃ©dit est comparÃ© au **token rÃ©el (vÃ©ritÃ© terrain)**.\n",
    "- Le modÃ¨le calcule la **perte par entropie croisÃ©e**, qui mesure lâ€™erreur de prÃ©diction.\n",
    "\n",
    "##### 4. RÃ©tropropagation et mise Ã  jour des poids\n",
    "\n",
    "- Ã€ partir de la perte, les gradients sont calculÃ©s pour tous les paramÃ¨tres du modÃ¨le (y compris les poids dâ€™attention, poids des FFN, embeddings).\n",
    "- La **descente de gradient** (ex. : avec Adam) met Ã  jour les poids pour rÃ©duire les erreurs futures.\n",
    "\n",
    "> ğŸ” **RÃ©pÃ©ter encore et encore avec des milliards de donnÃ©es dâ€™entraÃ®nement**\n",
    "\n",
    "#### Instruction du tuning\n",
    "\n",
    "![fine tunning](Figures/fine_tuning_0.png)\n",
    "\n",
    "![fine tunning](Figures/fine_tuning_1.png)\n",
    "\n",
    "![fine tunning](Figures/fine_tuning_2.png)\n",
    "\n",
    "Dans le tableau ci-dessus les lignes dont le *ParentId* est nul correspondent Ã  des *prompts* de type *question* et celles qui ont un *ParentId* non nul correspondent Ã  des *prompts* de type *complÃ©tion*/*sortie*.\n",
    "\n",
    "### Instruction\n",
    "\n",
    "ğŸ“˜ Exemple dâ€™Ã©chantillon d'entraÃ®nement\n",
    "Imaginons que lâ€™instruction soit :\n",
    "```yaml\n",
    "Instruction : Traduire en franÃ§ais  \n",
    "EntrÃ©e : The cat is sleeping.  \n",
    "Sortie attendue : Le chat dort.\n",
    "```\n",
    "\n",
    "ğŸ§  EntrÃ©e et sortie du modÃ¨le (sous forme de tokens)\n",
    "Lâ€™entrÃ©e complÃ¨te du modÃ¨le peut Ãªtre formatÃ©e comme suit :\n",
    "\n",
    "```text\n",
    "\"Translate to French\\nThe cat is sleeping.\\n\"\n",
    "```\n",
    "\n",
    "TokenisÃ©e en identifiants numÃ©riques :\n",
    "```text\n",
    "[101, 456, 23, 178, ..., 501]\n",
    "```\n",
    "\n",
    "Les tokens de sortie cibles sont :\n",
    "\n",
    "```text\n",
    "\"Le chat dort.\"  \n",
    "â†’ ['Le', 'Ä chat', 'Ä dort', '.']  \n",
    "â†’ [2345, 9876, 5432, 13]\n",
    "```\n",
    "\n",
    "### Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "L'objectif est de donner au modÃ¨le un sens des prioritÃ©s.\n",
    "\n",
    "![Reinforcement learning](Figures/reinforcement_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a414bec",
   "metadata": {},
   "source": [
    "## *Finetuning* et *prompt engineering*\n",
    "\n",
    "## ğŸ”§ Quâ€™est-ce que le Fine-Tuning ?\n",
    "\n",
    "Â« Le fine-tuning (ou ajustement fin) dâ€™un LLM consiste Ã  prendre un grand modÃ¨le de langage prÃ©-entraÃ®nÃ©  \n",
    "et Ã  poursuivre son entraÃ®nement sur un jeu de donnÃ©es plus petit et spÃ©cifique Ã  un domaine.  \n",
    "Ce processus met Ã  jour les paramÃ¨tres internes du modÃ¨le (poids) afin de lâ€™adapter  \n",
    "plus prÃ©cisÃ©ment Ã  des tÃ¢ches qui reflÃ¨tent les caractÃ©ristiques de ces nouvelles donnÃ©es. Â» â€” *GPT-4o*\n",
    "\n",
    "### ğŸ” Similaire Ã  lâ€™Instruction Tuning\n",
    "\n",
    "Â« Bien que le fine-tuning et lâ€™instruction tuning impliquent tous deux un entraÃ®nement spÃ©cifique Ã  une tÃ¢che,  \n",
    "lâ€™instruction tuning apprend au modÃ¨le Ã  suivre une grande variÃ©tÃ© dâ€™instructions,  \n",
    "tandis que le fine-tuning est souvent plus ciblÃ© et plus profondÃ©ment intÃ©grÃ© Ã  une tÃ¢che  \n",
    "ou un domaine particulier. Â»  â€” *GPT-4o*\n",
    "\n",
    "ğŸ§  Pourquoi faire du Fine-Tuning sur un LLM ?\n",
    "Â« Le fine-tuning permet Ã  un LLM de se spÃ©cialiser dans un domaine ou une tÃ¢che spÃ©cifique.\n",
    "Par exemple, vous pouvez l'ajuster sur les donnÃ©es internes de votre entreprise\n",
    "ou sur un ton de communication spÃ©cifique. Â»\n",
    "\n",
    "Â« Cela permet souvent dâ€™obtenir de bien meilleures performances sur les tÃ¢ches ciblÃ©es,\n",
    "car le modÃ¨le est exposÃ© Ã  des donnÃ©es plus pertinentes et apprend leurs schÃ©mas\n",
    "et subtilitÃ©s spÃ©cifiques. Â»\n",
    "\n",
    "âš¡ Pourquoi fine-tuner plutÃ´t que prÃ©-entraÃ®ner un LLM ?\n",
    "Â« L'entraÃ®nement dâ€™un modÃ¨le depuis zÃ©ro nÃ©cessite dâ€™Ã©normes ressources informatiques\n",
    "et un jeu de donnÃ©es massif et diversifiÃ©.\n",
    "\n",
    "Le fine-tuning, en revanche, revient Ã  donner un coup de pouce au modÃ¨le â€”\n",
    "il a dÃ©jÃ  appris les structures gÃ©nÃ©rales du langage.\n",
    "Le fine-tuning ne fait que concentrer son attention.\n",
    "Câ€™est donc beaucoup plus rentable et rapide, tout en offrant de bonnes performances\n",
    "pour un cas dâ€™usage spÃ©cifique. Â»\n",
    "\n",
    "Quand faut-il prÃ©-entraÃ®ner un LLM ?\n",
    "\n",
    "- **Langue nouvelle ou peu dotÃ©e en ressources**  \n",
    "  Lorsque la langue cible nâ€™est pas couverte par les modÃ¨les prÃ©-entraÃ®nÃ©s existants  \n",
    "  (ex. : un dialecte Ã©mergent ou une langue autochtone).\n",
    "\n",
    "- **Domaines trÃ¨s spÃ©cialisÃ©s ou peu explorÃ©s**  \n",
    "  Par exemple, des domaines avec un jargon technique pointu (juridique, biotechnologie, aÃ©rospatial)  \n",
    "  que les LLM nâ€™ont pas encore rencontrÃ©s (ex. : faible exposition sur Internet).\n",
    "\n",
    "- **AvancÃ©es architecturales ou mÃ©thodologiques**  \n",
    "  Lorsque vous souhaitez amÃ©liorer les capacitÃ©s dâ€™un LLM en modifiant son architecture,  \n",
    "  en utilisant de nouveaux jeux de donnÃ©es, ou en appliquant une mÃ©thode dâ€™apprentissage rÃ©cente  \n",
    "  (ex. : Mixture of Experts, Low-Rank Adaptation).\n",
    "\n",
    "Exemple de Fine tunning d'une LLM :\n",
    "\n",
    "![Reinforcement learning](Figures/fine_tunning_LLM_example.png)\n",
    "\n",
    "## Focus sur les RAG : embedding, Vector DB, Code architecture\n",
    "\n",
    "## ğŸ“š Principle: RAG vs Fine-Tuning\n",
    "\n",
    "**Principe** :  \n",
    "Le RAG (*Retrieval-Augmented Generation*) poursuit un objectif similaire au *fine-tuning*.\n",
    "Le but est de spÃ©cialiser le LLM dans un domaine ou une tÃ¢che prÃ©cise,  \n",
    "et lui donner accÃ¨s aux donnÃ©es de lâ€™entreprise.\n",
    "\n",
    "**DiffÃ©rence** :  \n",
    "Le RAG ne modifie pas les paramÃ¨tres du modÃ¨le LLM.  \n",
    "Il fait seulement **enrichir**/**augmenter** le prompt en y ajoutant des donnÃ©es externes pertinentes,  \n",
    "afin de fournir au LLM des connaissances et des exemples utiles Ã  lâ€™accomplissement de la tÃ¢che.\n",
    "\n",
    "![RAG](Figures/RAG.png)\n",
    "\n",
    "## Web application developpement\n",
    "\n",
    "## Code Walkthrough : Comment une application Flask est strucuturÃ©e ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d00c9b",
   "metadata": {},
   "source": [
    "## *Finetuning* et *prompt engineering*\n",
    "\n",
    "## ğŸ”§ Quâ€™est-ce que le Fine-Tuning ?\n",
    "\n",
    "Â« Le fine-tuning (ou ajustement fin) dâ€™un LLM consiste Ã  prendre un grand modÃ¨le de langage prÃ©-entraÃ®nÃ©  \n",
    "et Ã  poursuivre son entraÃ®nement sur un jeu de donnÃ©es plus petit et spÃ©cifique Ã  un domaine.  \n",
    "Ce processus met Ã  jour les paramÃ¨tres internes du modÃ¨le (poids) afin de lâ€™adapter  \n",
    "plus prÃ©cisÃ©ment Ã  des tÃ¢ches qui reflÃ¨tent les caractÃ©ristiques de ces nouvelles donnÃ©es. Â» â€” *GPT-4o*\n",
    "\n",
    "### ğŸ” Similaire Ã  lâ€™Instruction Tuning\n",
    "\n",
    "Â« Bien que le fine-tuning et lâ€™instruction tuning impliquent tous deux un entraÃ®nement spÃ©cifique Ã  une tÃ¢che,  \n",
    "lâ€™instruction tuning apprend au modÃ¨le Ã  suivre une grande variÃ©tÃ© dâ€™instructions,  \n",
    "tandis que le fine-tuning est souvent plus ciblÃ© et plus profondÃ©ment intÃ©grÃ© Ã  une tÃ¢che  \n",
    "ou un domaine particulier. Â»  â€” *GPT-4o*\n",
    "\n",
    "ğŸ§  Pourquoi faire du Fine-Tuning sur un LLM ?\n",
    "Â« Le fine-tuning permet Ã  un LLM de se spÃ©cialiser dans un domaine ou une tÃ¢che spÃ©cifique.\n",
    "Par exemple, vous pouvez l'ajuster sur les donnÃ©es internes de votre entreprise\n",
    "ou sur un ton de communication spÃ©cifique. Â»\n",
    "\n",
    "Â« Cela permet souvent dâ€™obtenir de bien meilleures performances sur les tÃ¢ches ciblÃ©es,\n",
    "car le modÃ¨le est exposÃ© Ã  des donnÃ©es plus pertinentes et apprend leurs schÃ©mas\n",
    "et subtilitÃ©s spÃ©cifiques. Â»\n",
    "\n",
    "âš¡ Pourquoi fine-tuner plutÃ´t que prÃ©-entraÃ®ner un LLM ?\n",
    "Â« L'entraÃ®nement dâ€™un modÃ¨le depuis zÃ©ro nÃ©cessite dâ€™Ã©normes ressources informatiques\n",
    "et un jeu de donnÃ©es massif et diversifiÃ©.\n",
    "\n",
    "Le fine-tuning, en revanche, revient Ã  donner un coup de pouce au modÃ¨le â€”\n",
    "il a dÃ©jÃ  appris les structures gÃ©nÃ©rales du langage.\n",
    "Le fine-tuning ne fait que concentrer son attention.\n",
    "Câ€™est donc beaucoup plus rentable et rapide, tout en offrant de bonnes performances\n",
    "pour un cas dâ€™usage spÃ©cifique. Â»\n",
    "\n",
    "Quand faut-il prÃ©-entraÃ®ner un LLM ?\n",
    "\n",
    "- **Langue nouvelle ou peu dotÃ©e en ressources**  \n",
    "  Lorsque la langue cible nâ€™est pas couverte par les modÃ¨les prÃ©-entraÃ®nÃ©s existants  \n",
    "  (ex. : un dialecte Ã©mergent ou une langue autochtone).\n",
    "\n",
    "- **Domaines trÃ¨s spÃ©cialisÃ©s ou peu explorÃ©s**  \n",
    "  Par exemple, des domaines avec un jargon technique pointu (juridique, biotechnologie, aÃ©rospatial)  \n",
    "  que les LLM nâ€™ont pas encore rencontrÃ©s (ex. : faible exposition sur Internet).\n",
    "\n",
    "- **AvancÃ©es architecturales ou mÃ©thodologiques**  \n",
    "  Lorsque vous souhaitez amÃ©liorer les capacitÃ©s dâ€™un LLM en modifiant son architecture,  \n",
    "  en utilisant de nouveaux jeux de donnÃ©es, ou en appliquant une mÃ©thode dâ€™apprentissage rÃ©cente  \n",
    "  (ex. : Mixture of Experts, Low-Rank Adaptation).\n",
    "\n",
    "Exemple de Fine tunning d'une LLM :\n",
    "\n",
    "![Reinforcement learning](Figures/fine_tunning_LLM_example.png)\n",
    "\n",
    "## Focus sur les RAG : embedding, Vector DB, Code architecture\n",
    "\n",
    "## ğŸ“š Principle: RAG vs Fine-Tuning\n",
    "\n",
    "**Principe** :  \n",
    "Le RAG (*Retrieval-Augmented Generation*) poursuit un objectif similaire au *fine-tuning*.\n",
    "Le but est de spÃ©cialiser le LLM dans un domaine ou une tÃ¢che prÃ©cise,  \n",
    "et lui donner accÃ¨s aux donnÃ©es de lâ€™entreprise.\n",
    "\n",
    "**DiffÃ©rence** :  \n",
    "Le RAG ne modifie pas les paramÃ¨tres du modÃ¨le LLM.  \n",
    "Il fait seulement **enrichir**/**augmenter** le prompt en y ajoutant des donnÃ©es externes pertinentes,  \n",
    "afin de fournir au LLM des connaissances et des exemples utiles Ã  lâ€™accomplissement de la tÃ¢che.\n",
    "\n",
    "![RAG](Figures/RAG.png)\n",
    "\n",
    "```python\n",
    "def retrieve_and_generate(question, vectordb):\n",
    "    \"\"\"\n",
    "    Answer a 'question' based on the most similar context from the \n",
    "    vector database 'vectordb'\n",
    "    \"\"\"\n",
    "    # Find data in the vectordb similar to the question\n",
    "    context = retrieve_similar_documents(\n",
    "        question,\n",
    "        vectordb,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Create a chat completion using the question and retrieved context\n",
    "        response = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"\n",
    "                  Answer the question based on the context below,\n",
    "                  and if the question can't be answered based on the context,\n",
    "                  say 'I don't know'\n",
    "                  \"\"\"\n",
    "                }, \n",
    "                {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                  Context:\n",
    "                  {context}\n",
    "\n",
    "                  Question:\n",
    "                  {question}\n",
    "\n",
    "                  Answer:\n",
    "                  \"\"\"}\n",
    "              ]\n",
    "          )\n",
    "      return response\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Option 1: Using DataFrame + cosine similarity\n",
    "def retrieve_similar_documents(question, df):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the top 3 most similar documents from the dataframe\n",
    "    Calcul de la distance Ã  la volÃ©e\n",
    "    \"\"\"\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = client.embeddings.create(\n",
    "        input=question, \n",
    "        engine=\"text-embedding-ada-002\"\n",
    "    )['data'][0]['embedding']\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = distances_from_embeddings(\n",
    "        q_embeddings,\n",
    "        df['embeddings'].values,\n",
    "        distance_metric='cosine'\n",
    "    )\n",
    "\n",
    "    df_result = df.sort_values('distances', ascending=True).head(3)\n",
    "\n",
    "    # Return the context\n",
    "    return \" \".join([doc for doc in df_result.document])\n",
    "\n",
    "# Option 2: Using a vector database query\n",
    "def retrieve_similar_documents(question, vectordb):\n",
    "    # Use the query method to retrieve the top 3 most similar documents\n",
    "    results = vectordb.query(\n",
    "        query_texts=[question],\n",
    "        n_results=3\n",
    "    )\n",
    "\n",
    "    # Concatenate the retrieved documents\n",
    "    context = \" \".join([doc for doc in results])\n",
    "\n",
    "    return context\n",
    "```\n",
    "\n",
    "#### DiffÃ©rence entre une base de donnÃ©es vectorielle et une base de donnÃ©es *standard* ?\n",
    "\n",
    "##### Recherche de similaritÃ© efficace\n",
    "\n",
    "- Les **vector stores** (magasins de vecteurs) sont conÃ§us pour la recherche **rapide de voisins les plus proches approximatifs (ANN)** dans des espaces de grande dimension (par ex. : embeddings de 384 ou 768 dimensions).\n",
    "- Ils permettent aux systÃ¨mes de trouver les rÃ©sultats **sÃ©mantiquement les plus similaires** plutÃ´t que de simples correspondances de mots-clÃ©s.\n",
    "- Essentiel pour les **applications IA/ML** comme la recherche sÃ©mantique, les moteurs de recommandation et **le RAG** (retrieval-augmented generation).\n",
    "\n",
    "**Exemple** : Vous cherchez Â« Comment payer ses impÃ´ts ? Â» â€” un vector store peut renvoyer un document intitulÃ© Â« Guide de dÃ©claration de revenus Â» car il est **sÃ©mantiquement similaire**, mÃªme si les mots ne correspondent pas exactement.\n",
    "\n",
    "##### Passage Ã  lâ€™Ã©chelle\n",
    "\n",
    "- Les **vector stores** sont conÃ§us pour **passer Ã  lâ€™Ã©chelle sur des millions ou milliards de vecteurs**, tout en renvoyant des rÃ©sultats en quelques millisecondes.\n",
    "- Ils prennent souvent en charge une **architecture distribuÃ©e** et lâ€™**accÃ©lÃ©ration matÃ©rielle** (ex. : GPU, SIMD).\n",
    "- Cela les rend particuliÃ¨rement adaptÃ©s aux applications telles que les **chatbots**, la **recherche documentaire**, et les **bases de connaissances IA Ã  grande Ã©chelle**.\n",
    "\n",
    "## Web application developpement : Front-end et Back-ends\n",
    "### API Principles\n",
    "\n",
    "![API Principles](Figures/API_principles.png)\n",
    "\n",
    "Exemple de structure d'une API :\n",
    "\n",
    "`app.py` :\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Dummy function to simulate getting car details\n",
    "def get_car_details(departure, destination):\n",
    "    # This is where you'd implement the logic to get the car details.\n",
    "    # For this example, we'll just return some hardcoded data.\n",
    "    return {\n",
    "        \"type_of_car\": \"UberX\",\n",
    "        \"price\": \"$10-13\",\n",
    "        \"gps_coordinates\": {\"lat\": 40.7128, \"long\": -74.0060}\n",
    "    }\n",
    "\n",
    "@app.route('/get_car', methods=['GET'])\n",
    "def get_car():\n",
    "    departure_address = request.args.get('departure_address')\n",
    "    destination_address = request.args.get('destination_address')\n",
    "\n",
    "    car_details = get_car_details(departure_address, destination_address)\n",
    "    return jsonify(car_details)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "`index.html` :\n",
    "\n",
    "```html\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Ride Details</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <h2>Get Ride Details</h2>\n",
    "\n",
    "    <label for=\"departure_address\">Departure Address:</label>\n",
    "    <input type=\"text\" id=\"departure_address\" name=\"departure_address\"><br><br>\n",
    "\n",
    "    <label for=\"destination_address\">Destination Address:</label>\n",
    "    <input type=\"text\" id=\"destination_address\" name=\"destination_address\"><br><br>\n",
    "\n",
    "    <button onclick=\"getRideDetails()\">Get Details</button>\n",
    "\n",
    "    <h3>Ride Details</h3>\n",
    "    <div id=\"rideDetails\"></div>\n",
    "\n",
    "    <script>\n",
    "        function getRideDetails() {\n",
    "            var departureAddress = document.getElementById('departure_address').value;\n",
    "            var destinationAddress = document.getElementById('destination_address').value;\n",
    "\n",
    "            fetch(`/get_car?departure_address=${departureAddress}&destination_address=${destinationAddress}`)\n",
    "                .then(response => response.json())\n",
    "                .then(data => {\n",
    "                    var details = `\n",
    "                        <p>Type of Car: ${data.type_of_car}</p>\n",
    "                        <p>Price: ${data.price}</p>\n",
    "                        <p>GPS Coordinates: lat ${data.gps_coordinates.lat} long ${data.gps_coordinates.long}</p>\n",
    "                    `;\n",
    "                    document.getElementById('rideDetails').innerHTML = details;\n",
    "                })\n",
    "                .catch(error => console.error('Error:', error));\n",
    "        }\n",
    "    </script>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "CÃ´tÃ© backend on a une route par laquelle on va pouvoir afficher le rÃ©sultat du code html.\n",
    "\n",
    "```python\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "```\n",
    "\n",
    "# ğŸ“ Flask Project Overview\n",
    "\n",
    "## Root Directory: `YourFlaskApp/`\n",
    "\n",
    "- `app.py`  \n",
    "  â†’ Main Flask application file.\n",
    "\n",
    "- `requirements.txt`  \n",
    "  â†’ Lists Python dependencies for the project.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Static Files: `static/`\n",
    "\n",
    "- `css/style.css`  \n",
    "  â†’ Custom stylesheets.\n",
    "\n",
    "- `js/script.js`  \n",
    "  â†’ Client-side JavaScript logic.\n",
    "\n",
    "- `images/logo.png`  \n",
    "  â†’ Image assets (e.g., logos, icons).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Templates: `templates/`\n",
    "\n",
    "- `index.html`  \n",
    "  â†’ Main homepage template.\n",
    "\n",
    "- `layout.html`  \n",
    "  â†’ Base layout template used with `{% extends \"layout.html\" %}`.\n",
    "\n",
    "- `other_template.html`  \n",
    "  â†’ Any additional HTML template for other views/pages.\n",
    "\n",
    "\n",
    "## Code Walkthrough : Comment une application Flask est strucuturÃ©e ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ede48b",
   "metadata": {},
   "source": [
    "# Projet\n",
    "\n",
    "CrÃ©er une application avec OpenAI et Cursor\n",
    "\n",
    "Ce projet explore le dÃ©veloppement d'une application web basÃ©e sur OpenAI API pour crÃ©er des features d'IA gÃ©nÃ©rative."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
